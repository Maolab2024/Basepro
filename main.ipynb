{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ff68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: extract.py [-h] [--toks_per_batch TOKS_PER_BATCH]\n",
      "                  [--repr_layers REPR_LAYERS [REPR_LAYERS ...]] --include\n",
      "                  {mean,per_tok,bos,contacts}\n",
      "                  [{mean,per_tok,bos,contacts} ...]\n",
      "                  [--truncation_seq_length TRUNCATION_SEQ_LENGTH] [--nogpu]\n",
      "                  [--concatenate_dir CONCATENATE_DIR]\n",
      "                  model_location fasta_file output_dir\n",
      "\n",
      "Extract per-token representations and model outputs for sequences in a FASTA\n",
      "file\n",
      "\n",
      "positional arguments:\n",
      "  model_location        PyTorch model file OR name of pretrained model to\n",
      "                        download (see README for models)\n",
      "  fasta_file            FASTA file on which to extract representations\n",
      "  output_dir            output directory for extracted representations\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --toks_per_batch TOKS_PER_BATCH\n",
      "                        maximum batch size\n",
      "  --repr_layers REPR_LAYERS [REPR_LAYERS ...]\n",
      "                        layers indices from which to extract representations\n",
      "                        (0 to num_layers, inclusive)\n",
      "  --include {mean,per_tok,bos,contacts} [{mean,per_tok,bos,contacts} ...]\n",
      "                        specify which representations to return\n",
      "  --truncation_seq_length TRUNCATION_SEQ_LENGTH\n",
      "                        truncate sequences longer than the given value\n",
      "  --nogpu               Do not use GPU even if available\n",
      "  --concatenate_dir CONCATENATE_DIR\n",
      "                        output directory for concatenated representations\n"
     ]
    }
   ],
   "source": [
    "!python src/esm/extract.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214a8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download over\n",
      "Transferred model to GPU\n",
      "Read data/p1450.fasta with 3 sequences\n",
      "Processing 1 of 2 batches (2 sequences)\n",
      "Device: cuda:0\n",
      "Processing 2 of 2 batches (1 sequences)\n",
      "Device: cuda:0\n",
      "Saved representations to data/esm_embedings/P1450\n",
      "/data/home/maorunzegroup/Basepro/src/esm/extract.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  file_data = torch.load(file_path)\n",
      "Shape of concatenated DataFrame: (3, 1280)\n",
      "Saved concatenated representations to /data/home/maorunzegroup/Basepro/data/esm_embedings/p1450_esm1b_t33_650M_UR50S.csv\n"
     ]
    }
   ],
   "source": [
    "!python src/esm/extract.py esm1b_t33_650M_UR50S data/p1450.fasta data/esm_embedings/P1450 --toks_per_batch 512 --include mean --concatenate_dir /data/home/maorunzegroup/Basepro/data/esm_embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb7b75",
   "metadata": {},
   "source": [
    "### round_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4fef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c84f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_csv(input_file_path,saved_file_path,sample_size=200):\n",
    "    \"\"\"\n",
    "    Randomly samples rows from a large CSV file and saves to a new file as round0 data.\n",
    "    \n",
    "    Parameters:\n",
    "    input_file_path (str): Path to input CSV file\n",
    "    saved_file_path (str): Path to save the sampled CSV file\n",
    "    sample_size (int): Number of rows to sample (default: 200)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        print(f\"Reading file: {os.path.basename(input_file_path)}...\")\n",
    "        df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Validate file size\n",
    "        if len(df) < sample_size:\n",
    "            print(f\"Warning: File has only {len(df)} rows, less than requested sample size {sample_size}\")\n",
    "            sample_size = len(df)\n",
    "        \n",
    "        # Perform random sampling\n",
    "        np.random.seed(42)  \n",
    "        round0_indices = np.random.choice(len(df), size=sample_size, replace=False)\n",
    "\n",
    "        sampled_df = pd.DataFrame()\n",
    "        sampled_df['variant'] = df['variant'][round0_indices]  # Fixed seed for reproducibility\n",
    "        sampled_df['fitness'] = df['fitness'][round0_indices]\n",
    "        sampled_df['indices'] = round0_indices\n",
    "        # Save sampled data\n",
    "        sampled_df.to_csv(saved_file_path, index=False)\n",
    "        print(f\"✓ Sampling complete! Saved to: {saved_file_path}\")\n",
    "        print(f\"Original rows: {len(df)}, Sampled rows: {len(sampled_df)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"Operation failed. Please check file path and format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54f6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: fitness.csv...\n",
      "✓ Sampling complete! Saved to: rounds_data/GB1/GB1_round_0.csv\n",
      "Original rows: 149361, Sampled rows: 200\n"
     ]
    }
   ],
   "source": [
    "random_sample_csv('data/GB1/fitness.csv', 'rounds_data/GB1/GB1_round_0.csv', sample_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb2bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import run_directed_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccb3eb",
   "metadata": {},
   "source": [
    "### round_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44164789",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_name = 'GB1'\n",
    "embeddings_base_path = 'data/GB1'\n",
    "embeddings_file_name = 'ESM2_x.pt'\n",
    "round_base_path = 'rounds_data/GB1'\n",
    "\n",
    "number_of_variants = 90\n",
    "output_dir = 'output'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed02812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_base_path = 'rounds_data/GB1'\n",
    "round_name = 'round_1'\n",
    "round_data_filenames = [\n",
    "    'GB1_round_0.csv',\n",
    "    # 'GB1_round_1.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e6c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = pd.read_csv('data/GB1/fitness.csv')\n",
    "all_variants = pd.DataFrame({\n",
    "    'variant': fitness['variant'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2ee0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载: GB1_round_0.csv (轮次 0)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import re   \n",
    "def load_round_data(round_base_path: str, round_file_names, protein_name: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load round data from CSV files in round order (round0, round1, ...).\n",
    "    \n",
    "    Parameters:\n",
    "    protein_name (str): Name of the protein \n",
    "    round_base_path (str): Base path for round data\n",
    "    \n",
    "    Returns:\n",
    "    list: Combined DataFrame from all CSV files in the round, in round order\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    # 收集所有匹配的文件\n",
    "    for file_name in round_file_names:\n",
    "        if file_name.startswith(protein_name) and file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(round_base_path, file_name)\n",
    "            all_files.append(file_path)\n",
    "    \n",
    "    # 按轮次排序的关键步骤\n",
    "    def extract_round_number(file_path):\n",
    "        \"\"\"从文件名中提取轮次数字\"\"\"\n",
    "        # 使用正则表达式匹配 roundX 模式\n",
    "        match = re.search(r'round_(\\d+)', file_path)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        # 如果文件名中没有轮次信息，返回 -1 放在最前面\n",
    "        return -1\n",
    "    \n",
    "    # 按轮次数字排序\n",
    "    sorted_files = sorted(all_files, key=extract_round_number,reverse = False)\n",
    "    \n",
    "    # 按顺序加载数据\n",
    "    all_round_data = []\n",
    "    for file_path in sorted_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_round_data.append(df)\n",
    "        print(f\"已加载: {os.path.basename(file_path)} (轮次 {extract_round_number(file_path)})\")\n",
    "    \n",
    "    return all_round_data\n",
    "all_round_data = load_round_data(round_base_path, round_data_filenames, protein_name)\n",
    "all_round_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GB1 - round_1\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/maorunzegroup/Basepro/src/data.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(file_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from data/GB1/ESM2_x.pt with shape torch.Size([149361, 5120])\n",
      "Embeddings loaded: torch.Size([149361, 5120])\n",
      "已加载: GB1_round_0.csv (轮次 0)\n",
      "torch.Size([200, 5120]) torch.Size([200]) (200,)\n",
      "successfully select 90 new variants for next round:\n",
      "       variant   fitness  indices\n",
      "104538    WYAG  2.455739   104538\n",
      "82283     YIAG  2.291723    82283\n",
      "30244     WFAG  2.289244    30244\n",
      "80659     YFAG  2.273627    80659\n",
      "35767     TIAG  2.248921    35767\n",
      "...        ...       ...      ...\n",
      "115498    IGAG  1.632279   115498\n",
      "7548      LGAG  1.628683     7548\n",
      "20533     KVAG  1.624723    20533\n",
      "10104     IVGG  1.621714    10104\n",
      "77161     KICG  1.621399    77161\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "\n",
      "Top 90 variants predicted by the modelf or next round: 90\n",
      "       variant   fitness  indices\n",
      "104538    WYAG  2.455739   104538\n",
      "82283     YIAG  2.291723    82283\n",
      "30244     WFAG  2.289244    30244\n",
      "80659     YFAG  2.273627    80659\n",
      "35767     TIAG  2.248921    35767\n",
      "...        ...       ...      ...\n",
      "115498    IGAG  1.632279   115498\n",
      "7548      LGAG  1.628683     7548\n",
      "20533     KVAG  1.624723    20533\n",
      "10104     IVGG  1.621714    10104\n",
      "77161     KICG  1.621399    77161\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "\n",
      "Data saved to output/GB1/round_1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m this_round_variants, df_test, df_sorted_all \u001b[38;5;241m=\u001b[39m run_directed_evolution(\n\u001b[1;32m      2\u001b[0m     protein_name,\n\u001b[1;32m      3\u001b[0m     round_name,\n\u001b[1;32m      4\u001b[0m     embeddings_base_path,\n\u001b[1;32m      5\u001b[0m     embeddings_file_name,\n\u001b[1;32m      6\u001b[0m     round_base_path,\n\u001b[1;32m      7\u001b[0m     round_data_filenames,\n\u001b[1;32m      8\u001b[0m     number_of_variants,\n\u001b[1;32m      9\u001b[0m     output_dir,\n\u001b[1;32m     10\u001b[0m     regression_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     all_variants\u001b[38;5;241m=\u001b[39mall_variants\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "df_next_round, df_pre_all_sorted = run_directed_evolution(\n",
    "    protein_name,\n",
    "    round_name,\n",
    "    embeddings_base_path,\n",
    "    embeddings_file_name,\n",
    "    round_base_path,\n",
    "    round_data_filenames,\n",
    "    number_of_variants,\n",
    "    output_dir,\n",
    "    regression_model='xgboost',\n",
    "    all_variants=all_variants\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5105523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>fitness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMHG</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QPEI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMYW</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KWNA</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QDRA</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant   fitness\n",
       "0    AMHG  0.000000\n",
       "1    QPEI  0.000000\n",
       "2    GMYW  0.000000\n",
       "3    KWNA  0.001791\n",
       "4    QDRA  0.004730"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness = pd.read_csv('data/GB1/fitness.csv')\n",
    "fitness.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "37c564e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/maorunzegroup/Basepro/src/data.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(file_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from src.data import load_embeddings\n",
    "embeddings = load_embeddings(embeddings_base_path, embeddings_file_name,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "756ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4c1c0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载: GB1_round_0.csv (轮次 0)\n",
      "已加载: GB1_round_1.csv (轮次 1)\n"
     ]
    }
   ],
   "source": [
    "def load_round_data(round_base_path: str, protein_name: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load round data from CSV files in round order (round0, round1, ...).\n",
    "    \n",
    "    Parameters:\n",
    "    protein_name (str): Name of the protein \n",
    "    round_base_path (str): Base path for round data\n",
    "    \n",
    "    Returns:\n",
    "    list: Combined DataFrame from all CSV files in the round, in round order\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    # 收集所有匹配的文件\n",
    "    for file_name in round_data_filenames:\n",
    "        if file_name.startswith(protein_name) and file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(round_base_path, file_name)\n",
    "            all_files.append(file_path)\n",
    "    \n",
    "    # 按轮次排序的关键步骤\n",
    "    def extract_round_number(file_path):\n",
    "        \"\"\"从文件名中提取轮次数字\"\"\"\n",
    "        # 使用正则表达式匹配 roundX 模式\n",
    "        match = re.search(r'round_(\\d+)', file_path)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        # 如果文件名中没有轮次信息，返回 -1 放在最前面\n",
    "        return -1\n",
    "    \n",
    "    # 按轮次数字排序\n",
    "    sorted_files = sorted(all_files, key=extract_round_number,reverse = False)\n",
    "    \n",
    "    # 按顺序加载数据\n",
    "    all_round_data = []\n",
    "    for file_path in sorted_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_round_data.append(df)\n",
    "        print(f\"已加载: {os.path.basename(file_path)} (轮次 {extract_round_number(file_path)})\")\n",
    "    \n",
    "    return all_round_data\n",
    "\n",
    "all_round_data = load_round_data(round_base_path, protein_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a3b52422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_round_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "94a38eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149361"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "732dd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_y = []\n",
    "list_indices = []\n",
    "\n",
    "for df in all_round_data:\n",
    "    X_round = embeddings[df['indices'].values]\n",
    "    y_round = torch.tensor(df['fitness'].values,dtype=torch.float32)\n",
    "    round_indices = df['indices']\n",
    "\n",
    "    all_X.append(X_round)\n",
    "    all_y.append(y_round)\n",
    "    list_indices.append(round_indices)\n",
    "X_train = torch.cat(all_X, dim=0)\n",
    "y_train = torch.cat(all_y, dim=0) \n",
    "\n",
    "train_indices = pd.concat(list_indices, ignore_index=True)\n",
    "test_indices = np.array([i for i in range(len(fitness['variant'])) if i not in train_indices])\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "92637c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       96399\n",
       "1       68091\n",
       "2       87476\n",
       "3        1108\n",
       "4      129203\n",
       "        ...  \n",
       "285    115498\n",
       "286      7548\n",
       "287     20533\n",
       "288     10104\n",
       "289     77161\n",
       "Name: indices, Length: 290, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = embeddings[all_round_data[0]['indices'].values]\n",
    "# y_train = torch.tensor(all_round_data[0]['fitness'].values,dtype=torch.float32)\n",
    "# X_train = X_train.to(device)\n",
    "# y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d49aeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 训练XGBoost模型\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100, \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    device = 'cuda'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. 预测所有1000个样本的fitness\n",
    "all_predictions = model.predict(embeddings)\n",
    "\n",
    "# 7. 评估结果 (在训练集和测试集上)\n",
    "train_predictions = all_predictions[train_indices]\n",
    "test_predictions = all_predictions[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f975ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_all= pd.DataFrame({\n",
    "    'variant': fitness['variant'],\n",
    "    'fitness': all_predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f60539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d65b2a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>fitness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142720</th>\n",
       "      <td>YHAG</td>\n",
       "      <td>2.649550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104538</th>\n",
       "      <td>WYAG</td>\n",
       "      <td>2.458590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82283</th>\n",
       "      <td>YIAG</td>\n",
       "      <td>2.294217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30244</th>\n",
       "      <td>WFAG</td>\n",
       "      <td>2.293822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80659</th>\n",
       "      <td>YFAG</td>\n",
       "      <td>2.279350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>SRCH</td>\n",
       "      <td>-0.017356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>CLNH</td>\n",
       "      <td>-0.017850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108291</th>\n",
       "      <td>DVAK</td>\n",
       "      <td>-0.018296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21100</th>\n",
       "      <td>KLCH</td>\n",
       "      <td>-0.018333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145282</th>\n",
       "      <td>GKCH</td>\n",
       "      <td>-0.023446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149361 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       variant   fitness\n",
       "142720    YHAG  2.649550\n",
       "104538    WYAG  2.458590\n",
       "82283     YIAG  2.294217\n",
       "30244     WFAG  2.293822\n",
       "80659     YFAG  2.279350\n",
       "...        ...       ...\n",
       "3441      SRCH -0.017356\n",
       "18521     CLNH -0.017850\n",
       "108291    DVAK -0.018296\n",
       "21100     KLCH -0.018333\n",
       "145282    GKCH -0.023446\n",
       "\n",
       "[149361 rows x 2 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre_all_sorted = df_pre_all.sort_values(by='fitness', ascending=False)\n",
    "df_pre_all_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "440fe837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功选择了 90 个新的变异体:\n",
      "       variant   fitness  indices\n",
      "130866    YDAG  2.034570   130866\n",
      "32933     YEAG  2.018511    32933\n",
      "24555     YTAG  2.017959    24555\n",
      "113590    YNAG  2.013072   113590\n",
      "87302     WDAG  2.003016    87302\n",
      "...        ...       ...      ...\n",
      "21808     ALIG  1.777380    21808\n",
      "70299     GKPG  1.776510    70299\n",
      "51900     WPVG  1.776355    51900\n",
      "48704     ITAG  1.776309    48704\n",
      "29855     EIKG  1.774183    29855\n",
      "\n",
      "[90 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "number_variant = 90\n",
    "\n",
    "filtered_df = df_pre_all_sorted[~df_pre_all_sorted.index.isin(train_indices)]\n",
    "\n",
    "# 3. 取前 number_variant 个变异体\n",
    "selected_variants = filtered_df.head(number_variant)\n",
    "\n",
    "# 4. 如果需要，可以将结果保存到新的 DataFrame\n",
    "result_df = selected_variants[['variant', 'fitness']].copy()\n",
    "result_df['indices'] = selected_variants.index  # 保存原始索引\n",
    "\n",
    "# 显示结果\n",
    "print(f\"成功选择了 {len(selected_variants)} 个新的变异体:\")\n",
    "print(result_df)\n",
    "\n",
    "# # 5. 保存为 CSV 文件\n",
    "# filepath = os.path.join(round_base_path, f\"{protein_name}_{round_name}.csv\")\n",
    "# result_df.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evolvepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
