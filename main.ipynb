{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ff68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: extract.py [-h] [--toks_per_batch TOKS_PER_BATCH]\n",
      "                  [--repr_layers REPR_LAYERS [REPR_LAYERS ...]] --include\n",
      "                  {mean,per_tok,bos,contacts}\n",
      "                  [{mean,per_tok,bos,contacts} ...]\n",
      "                  [--truncation_seq_length TRUNCATION_SEQ_LENGTH] [--nogpu]\n",
      "                  [--concatenate_dir CONCATENATE_DIR]\n",
      "                  model_location fasta_file output_dir\n",
      "\n",
      "Extract per-token representations and model outputs for sequences in a FASTA\n",
      "file\n",
      "\n",
      "positional arguments:\n",
      "  model_location        PyTorch model file OR name of pretrained model to\n",
      "                        download (see README for models)\n",
      "  fasta_file            FASTA file on which to extract representations\n",
      "  output_dir            output directory for extracted representations\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --toks_per_batch TOKS_PER_BATCH\n",
      "                        maximum batch size\n",
      "  --repr_layers REPR_LAYERS [REPR_LAYERS ...]\n",
      "                        layers indices from which to extract representations\n",
      "                        (0 to num_layers, inclusive)\n",
      "  --include {mean,per_tok,bos,contacts} [{mean,per_tok,bos,contacts} ...]\n",
      "                        specify which representations to return\n",
      "  --truncation_seq_length TRUNCATION_SEQ_LENGTH\n",
      "                        truncate sequences longer than the given value\n",
      "  --nogpu               Do not use GPU even if available\n",
      "  --concatenate_dir CONCATENATE_DIR\n",
      "                        output directory for concatenated representations\n"
     ]
    }
   ],
   "source": [
    "!python src/esm/extract.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214a8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download over\n",
      "Transferred model to GPU\n",
      "Read data/p1450.fasta with 3 sequences\n",
      "Processing 1 of 2 batches (2 sequences)\n",
      "Device: cuda:0\n",
      "Processing 2 of 2 batches (1 sequences)\n",
      "Device: cuda:0\n",
      "Saved representations to data/esm_embedings\n",
      "/data/home/maorunzegroup/Basepro/src/esm/extract.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  file_data = torch.load(file_path)\n",
      "Shape of concatenated DataFrame: (3, 1280)\n",
      "Saved concatenated representations to /data/home/maorunzegroup/Basepro/data/esm_embedings/p1450_esm1b_t33_650M_UR50S.csv\n"
     ]
    }
   ],
   "source": [
    "!python src/esm/extract.py esm1b_t33_650M_UR50S data/p1450.fasta data/esm_embedings --toks_per_batch 512 --include mean --concatenate_dir /data/home/maorunzegroup/Basepro/data/esm_embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde1fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc5930b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1281)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings=pd.read_csv('data/esm_embedings/p1450_esm1b_t33_650M_UR50S.csv')\n",
    "embedings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44164789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
